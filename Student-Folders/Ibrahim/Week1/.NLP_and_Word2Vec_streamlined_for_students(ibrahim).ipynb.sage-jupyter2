{"backend_state":"init","connection_file":"/projects/aebc210b-e912-4df7-91ea-37e0f8451ece/.local/share/jupyter/runtime/kernel-493b6d6f-63d8-4268-ae83-b6f0312d47db.json","kernel":"nlp_env","kernel_error":"","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"metadata":{"interpreter":{"hash":"335ee12212264728feb72f243af72c5a8ea26c832f07e1f651ce9e17c7ceae23"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{},"version_major":2,"version_minor":0}}},"trust":true,"type":"settings"}
{"cell_type":"code","exec_count":0,"id":"180bfd","input":"'''\nDoing the same without removing stop words or lemming\n'''\n# tokenize the text using sent_tokenize\n\n# from this list of sentences, create a list of lists where the ith entry in the list is the word tokenizaiton of the ith sentence (after preprocessing)","pos":41,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"2b4814","input":"# Install NLTK - pip install nltk\nimport nltk\nnltk.download('wordnet')\nnltk.download('punkt')","pos":2,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"30457f","input":"# We will read the contents of the Wikipedia article \"Global_warming\" as an example, please feel free to use your own! You can use the url below:\nurl = 'https://en.wikipedia.org/wiki/Global_warming' # you can change this to use other sites as well.\n\n# We can open the page using \"urllib.request.urlopen\" then read it using \".read()\"\nsource = urllib.request.urlopen(url).read()\n\n# Beautiful Soup is a Python library for pulling data out of HTML and XML files.\n# you may need to install a parser library --> \"!pip3 install lxml\"\n# Parsing the data/creating BeautifulSoup object\n\nsoup = bs.BeautifulSoup(source,\"html.parser\") \n\n# Fetching the data\ntext = \"\"\nfor paragraph in soup.find_all('p'): #The <p> tag defines a paragraph in the webpages\n    text += paragraph.text\n\n# Preprocessing the data\n\ntext = re.sub(r'\\[[0-9]*\\]',' ',text) # [0-9]* --> Matches zero or more repetitions of any digit from 0 to 9\ntext = text.lower() #everything to lowercase\ntext = re.sub(r'\\W^.?!',' ',text) # \\W --> Matches any character which is not a word character except (.?!)\ntext = re.sub(r'\\d',' ',text) # \\d --> Matches any decimal digit\ntext = re.sub(r'\\s+',' ',text) # \\s --> Matches any characters that are considered whitespace (Ex: [\\t\\n\\r\\f\\v].)","pos":5,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"485e31","input":"from nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\n# try each of the words below\nstemmer.stem('troubled')\n#stemmer.stem('trouble')\n#stemmer.stem('troubling')\n#stemmer.stem('troubles')","pos":15,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"4bc5a8","input":"","pos":17,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"55c74b","input":"# code to print a wordcloud for your sentences\nwordcloud = WordCloud(\n                        background_color='white',\n                        max_words=100,\n                        max_font_size=50, \n                        random_state=42\n                        ).generate(str(sentences))\nfig = plt.figure(1)\nplt.figure(figsize=(10,10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","pos":38,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"589900","input":"''' Training the Word2Vec model. You should pass:\n1. a list of lists where the ith entry in the list is the word tokenizaiton of the ith sentence\n2. min_count=1 --> Ignores all words with total frequency lower than 1 (i.e., include everything).\n'''\n# create the model\n\n# get the most common words of the model (it's entire vocabulary)\n\n# save the model to use it later\n\n# model = Word2Vec.load(\"word2vec.model\")","pos":29,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"632bec","input":"","pos":46,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"6745a6","input":"# POS Tagging example\n# CC - coordinating conjunction\n# NN - noun, singular (cat, tree)\n#all_words = nltk.word_tokenize(text)  ###If we want to look at part of speech taking before we stem/lem\n\ntagged_words = nltk.pos_tag(all_words)\n##Creates a list of lists where each element of the list is [word,partofspeech abbreviation]\n\n# Tagged word paragraph\nword_tags = []\nfor tw in tagged_words:\n    word_tags.append(tw[0]+\"_\"+tw[1])\n\ntagged_paragraph = ' '.join(word_tags)\n\n'''\nYour code here: print the first 1000 characters of tagged_paragraph.\n'''","pos":23,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"7531c6","input":"### Finding the most similar words in the model but... you get the idea ###\n\n","pos":45,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"759b2f","input":"import urllib\nimport bs4 as bs\nimport re","pos":4,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"7c58b0","input":"# Finding a vector of a word, but badly","pos":44,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"7f1f42","input":"nltk.download('stopwords')\nfrom nltk.corpus import stopwords","pos":11,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"80d85e","input":"# Redo the word cloud but set stopwords to empty so it looks really bad\nwordcloud = WordCloud(\n                        background_color='white',\n                        max_words=100,\n                        max_font_size=50, \n                        random_state=42, ###SET STOPWORDS = [] and/or include_numbers = True or you will get the same thing!!!\n                        stopwords = [],\n                        include_numbers = True).generate(str(lame_sentences)) \nfig = plt.figure(1)\nplt.figure(figsize=(10,10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","pos":42,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"9b26a8","input":"# Look up the most similar words to certain words in your text using the model.wv.most_similar() function","pos":31,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"9f3998","input":"","pos":32,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"ab6ef6","input":"# Install gensim - pip install gensim\nimport nltk\nfrom gensim.models import Word2Vec\nimport matplotlib.pyplot as plt\nnltk.download('punkt')\nfrom wordcloud import WordCloud","pos":25,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"ace127","input":"print(sentences[:10]) ","pos":20,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"ae0301","input":"#print the first 10 most common words.","pos":30,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"c3424c","input":"nltk.download('averaged_perceptron_tagger')","pos":22,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"c63ee5","input":"","pos":36,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"ca1f18","input":"from nltk.stem import WordNetLemmatizer\n    \n## Step 1: Import the lemmatizer\nlemmatizer = WordNetLemmatizer()\n\n'''\nYour code here: Define a function called \"lem_sentences\" that: loops through the sentences, split the sentences up by words and applies \"lemmatizer.lemmatize\" to each word and then join everything back into a sentence\n'''\n##Similar to stopwords: For loop through the sentences, split by words and apply \"lemmatizer.lemmatize\" to each word and join back into a sentence\ndef lem_sentences(sentences):\n\n    return sentences\nsentences = lem_sentences(sentences)\nprint(sentences[:10]) ","pos":19,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"cbe7a2","input":"# Training the Word2Vec model (same code as before), but one change: use our lame data that was not preprocessed\n\n# Try printing this after training the model.\nwords = model.wv.index_to_key\nprint(words[:10])","pos":43,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"cd1861","input":"#Let's go ahead and create a list that's formatted how word2vec needs:\n    # a list of lists where the ith entry in the list is the word tokenizaiton of the ith sentence (after preprocessing)","pos":26,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"e4d7cb","input":"    ### Finding the most similar words in the model ###\n","pos":35,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"e5e5e1","input":"# reFetching the data\nlame_text = \"\"\nfor paragraph in soup.find_all('p'): #The <p> tag defines a paragraph in the webpages\n    lame_text += paragraph.text","pos":40,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"e61369","input":"similar1, similar2","pos":37,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"f1bb3b","input":"    # Finding Word Vectors - print word vectors for certain words in your text\n","pos":34,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"f5629b","input":"# print the tokenized list of lists","pos":27,"type":"cell"}
{"cell_type":"code","exec_count":1,"id":"e7cbcd","input":"import bs4 as bs\nimport nltk\nimport urllib\nimport re\nimport string as s\nnltk.download('wordnet')\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\n\npunc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\nurl = 'https://en.wikipedia.org/wiki/Global_warming'\nsource = urllib.request.urlopen(url).read()\nsoup = bs.BeautifulSoup(source,\"html.parser\") \n\ntext = \"\"\nfor paragraph in soup.find_all('p'):\n    text += paragraph.text\n    \n#Processing the data\ntext = re.sub(r'\\[[0-9]*\\]',' ',text) # [0-9]* --> Matches zero or more repetitions of any digit from 0 to 9\ntext = text.lower() #everything to lowercase\ntext = re.sub(r'\\W^.?!',' ',text) # \\W --> Matches any character which is not a word character except (.?!)\ntext = re.sub(r'\\d',' ',text) # \\d --> Matches any decimal digit\ntext = re.sub(r'\\s+',' ',text) # \\s --> Matches any characters that are considered whitespace (Ex: [\\t\\n\\r\\f\\v].)\n\ndef remove_punctuation(sentences):\n    \n    ### Some code goes here. Hint: Try looking up how to remove punctuation in NLTK if you get stuck. ###\n    for ele in sentences:\n        if ele in punc:\n            print(sentences.replace(ele, \"\"))\n\n\nsent = remove_punctuation(text)\nprint(sent[:10]) #eliminating all punctuation.","output":{"0":{"name":"stderr","output_type":"stream","text":"[nltk_data] Downloading package wordnet to /projects/aebc210b-e912-4df\n[nltk_data]     7-91ea-37e0f8451ece/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt to /projects/aebc210b-e912-4df7-\n[nltk_data]     91ea-37e0f8451ece/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to /project\n[nltk_data]     s/aebc210b-e912-4df7-91ea-37e0f8451ece/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package stopwords to /projects/aebc210b-e912-4\n[nltk_data]     df7-91ea-37e0f8451ece/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"},"1":{"name":"stderr","output_type":"stream","text":"WARNING: some intermediate output was truncated.\n"},"2":{"name":"stderr","output_type":"stream","text":"WARNING: 1 intermediate output message was discarded.\n"},"3":{"more_output":true}},"pos":13,"scrolled":true,"type":"cell"}
{"cell_type":"code","exec_count":10,"id":"29c45f","input":"import bs4 as bs\nimport nltk\nimport urllib\nimport re\nnltk.download('wordnet')\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\n# define a function called \"remove_stopwords\" that takes in a list of the sentences of the text and returns one that doesn't have any stopwords.\n\ndef remove_stopwords(sentences):\n    \n    ### Some code goes here. Hint: You may have to look up how to remove stopwords in NLTK if you get stuck. ###\n    \n    for i in range(len(sentences)):\n        words = nltk.word_tokenize(sentences[i])\n        words = [word for word in words if word not in \",.?!()\"]\n        sentences[i] = ' '.join(words)\n    return sentences\n\n\n\nurl = 'https://en.wikipedia.org/wiki/Global_warming'\nsource = urllib.request.urlopen(url).read()\nsoup = bs.BeautifulSoup(source,\"html.parser\") \n\ntext = \"\"\nfor paragraph in soup.find_all('p'):\n    text += paragraph.text\n    \n#Processing the data\ntext = re.sub(r'\\[[0-9]*\\]',' ',text) # [0-9]* --> Matches zero or more repetitions of any digit from 0 to 9\ntext = text.lower() #everything to lowercase\ntext = re.sub(r'\\W^.?!',' ',text) # \\W --> Matches any character which is not a word character except (.?!)\ntext = re.sub(r'\\d',' ',text) # \\d --> Matches any decimal digit\ntext = re.sub(r'\\s+',' ',text) # \\s --> Matches any characters that are considered whitespace (Ex: [\\t\\n\\r\\f\\v].)\n\n\nsentences = nltk.sent_tokenize(text)\nprint(sentences[:10])\n\n\n###Then actually apply your function###\nsentences = remove_stopwords(sentences)\nprint(sentences[:10]) ","output":{"0":{"name":"stderr","output_type":"stream","text":"[nltk_data] Downloading package wordnet to /projects/aebc210b-e912-4df\n[nltk_data]     7-91ea-37e0f8451ece/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt to /projects/aebc210b-e912-4df7-\n[nltk_data]     91ea-37e0f8451ece/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to /project\n[nltk_data]     s/aebc210b-e912-4df7-91ea-37e0f8451ece/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package stopwords to /projects/aebc210b-e912-4\n[nltk_data]     df7-91ea-37e0f8451ece/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"},"1":{"name":"stdout","output_type":"stream","text":"[\" contemporary climate change includes both global warming and its impacts on earth's weather patterns.\", 'there have been previous periods of climate change, but the current changes are distinctly more rapid and not due to natural causes.', 'instead, they are caused by the emission of greenhouse gases, mostly carbon dioxide (co ) and methane.', 'burning fossil fuels for energy production creates most of these emissions.', 'certain agricultural practices, industrial processes, and forest loss are additional sources.', 'as their name suggests, these gases trap heat from sunlight near the earth’s surface, warming it over time.', 'due to climate change, deserts are expanding, while heat waves and wildfires are becoming more common.', 'increased warming in the arctic has contributed to melting permafrost, glacial retreat and sea ice loss.', 'higher temperatures are also causing more intense storms, droughts, and other weather extremes.', 'rapid environmental change in mountains, coral reefs, and the arctic is forcing many species to relocate or become extinct.']\n[\"contemporary climate change includes both global warming and its impacts on earth 's weather patterns\", 'there have been previous periods of climate change but the current changes are distinctly more rapid and not due to natural causes', 'instead they are caused by the emission of greenhouse gases mostly carbon dioxide co and methane', 'burning fossil fuels for energy production creates most of these emissions', 'certain agricultural practices industrial processes and forest loss are additional sources', 'as their name suggests these gases trap heat from sunlight near the earth ’ s surface warming it over time', 'due to climate change deserts are expanding while heat waves and wildfires are becoming more common', 'increased warming in the arctic has contributed to melting permafrost glacial retreat and sea ice loss', 'higher temperatures are also causing more intense storms droughts and other weather extremes', 'rapid environmental change in mountains coral reefs and the arctic is forcing many species to relocate or become extinct']\n"}},"pos":12,"scrolled":true,"type":"cell"}
{"cell_type":"code","exec_count":2,"id":"754b68","input":"text[:100]","output":{"0":{"ename":"NameError","evalue":"name 'text' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-e506b87c8457>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"]}},"pos":6,"type":"cell"}
{"cell_type":"code","exec_count":55,"id":"a1b4cc","input":"#Your code here:\n#Define a function called \"stem_sentences\" that takes in a list of sentences and returns a list of stemmed sentences.\n\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\n\n\nps = PorterStemmer()\npunc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n\ntext = 'This is a sentence. Multiple sentences were harmed in the making of this sentence. I apologize to say this, but it seems you were too late. The sentence was already #created. I thank you for your consideration, yet it was all for nought. -Fairly.'\ntext = re.sub(r'\\[[0-9]*\\]',' ',text) # [0-9]* --> Matches zero or more repetitions of any digit from 0 to 9\ntext = text.lower() #everything to lowercase\ntext = re.sub(r'\\W^.?!',' ',text) # \\W --> Matches any character which is not a word character except (.?!)\ntext = re.sub(r'\\d',' ',text) # \\d --> Matches any decimal digit\ntext = re.sub(r'\\s+',' ',text) # \\s --> Matches any characters that are considered whitespace (Ex: [\\t\\n\\r\\f\\v].\n\ntokens = word_tokenize(text)\n\n\ndef stem_sentences(sentences):\n    stemmed = []\n\n    for token in tokens:\n         stemmed_word = ps.stem(token)\n         stemmed.append(stemmed_word)\n\n    print(stemmed)\n\n\nj = stem_sentences(text)\nprint(j)\n    \n\n\n\n\n\n#from nltk.stem import PorterStemmer\n#from nltk.tokenize import word_tokenize\n\n#text = 'This is a sentence. Multiple sentences were harmed in the making of this sentence. I apologize to say this, but it seems you were too late. The sentence was already #created. I thank you for your consideration, yet it was all for nought. -Fairly.'\n\n\n\n#words = nltk.word_tokenize(text)\n#stemmer = PorterStemmer()\n#def stem_sentences(sentences):\n#    p = stemmer.stem(sentences, True)\n#    print(p)\n#    \n#bob = stem_sentences(words)\n#print(bob)","output":{"0":{"name":"stdout","output_type":"stream","text":"['thi', 'is', 'a', 'sentenc', '.', 'multipl', 'sentenc', 'were', 'harm', 'in', 'the', 'make', 'of', 'thi', 'sentenc', '.', 'i', 'apolog', 'to', 'say', 'thi', ',', 'but', 'it', 'seem', 'you', 'were', 'too', 'late', '.', 'the', 'sentenc', 'wa', 'alreadi', '#', 'creat', '.', 'i', 'thank', 'you', 'for', 'your', 'consider', ',', 'yet', 'it', 'wa', 'all', 'for', 'nought', '.', '-fairli', '.']\nNone\n"}},"pos":16,"type":"cell"}
{"cell_type":"code","exec_count":8,"id":"c63646","input":"\n#Your code here: Tokenize the words from the data and set it to a variable called words.\n#Hint: how to this might be on the very home page of NLTK!\n\nimport bs4 as bs\nimport nltk\nimport urllib\nimport re\nnltk.download('wordnet')\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\n\nurl = 'https://en.wikipedia.org/wiki/Global_warming'\nsource = urllib.request.urlopen(url).read()\nsoup = bs.BeautifulSoup(source,\"html.parser\") \n\ntext = \"\"\nfor paragraph in soup.find_all('p'):\n    text += paragraph.text\n    \n#Processing the data\ntext = re.sub(r'\\[[0-9]*\\]',' ',text) # [0-9]* --> Matches zero or more repetitions of any digit from 0 to 9\ntext = text.lower() #everything to lowercase\ntext = re.sub(r'\\W^.?!',' ',text) # \\W --> Matches any character which is not a word character except (.?!)\ntext = re.sub(r'\\d',' ',text) # \\d --> Matches any decimal digit\ntext = re.sub(r'\\s+',' ',text) # \\s --> Matches any characters that are considered whitespace (Ex: [\\t\\n\\r\\f\\v].)\n\n\nwords = nltk.word_tokenize(text)\nprint(words[:10])\n\n","output":{"0":{"name":"stderr","output_type":"stream","text":"[nltk_data] Downloading package wordnet to /projects/aebc210b-e912-4df\n[nltk_data]     7-91ea-37e0f8451ece/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt to /projects/aebc210b-e912-4df7-\n[nltk_data]     91ea-37e0f8451ece/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to /project\n[nltk_data]     s/aebc210b-e912-4df7-91ea-37e0f8451ece/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n"},"1":{"name":"stdout","output_type":"stream","text":"['contemporary', 'climate', 'change', 'includes', 'both', 'global', 'warming', 'and', 'its', 'impacts']\n"}},"pos":8,"type":"cell"}
{"cell_type":"code","exec_count":9,"id":"9c3ed8","input":"\n#Your code here: Tokenize the sentences from the data  and set it to a variable called sentences.\n#Hint: try googling how to tokenize sentences in NLTK!\n\nimport bs4 as bs\nimport nltk\nimport urllib\nimport re\nnltk.download('wordnet')\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\n\nurl = 'https://en.wikipedia.org/wiki/Global_warming'\nsource = urllib.request.urlopen(url).read()\nsoup = bs.BeautifulSoup(source,\"html.parser\") \n\ntext = \"\"\nfor paragraph in soup.find_all('p'):\n    text += paragraph.text\n    \n#Processing the data\ntext = re.sub(r'\\[[0-9]*\\]',' ',text) # [0-9]* --> Matches zero or more repetitions of any digit from 0 to 9\ntext = text.lower() #everything to lowercase\ntext = re.sub(r'\\W^.?!',' ',text) # \\W --> Matches any character which is not a word character except (.?!)\ntext = re.sub(r'\\d',' ',text) # \\d --> Matches any decimal digit\ntext = re.sub(r'\\s+',' ',text) # \\s --> Matches any characters that are considered whitespace (Ex: [\\t\\n\\r\\f\\v].)\n\n\nsentences = nltk.sent_tokenize(text)\nprint(sentences[:10])\n\n","output":{"0":{"name":"stderr","output_type":"stream","text":"[nltk_data] Downloading package wordnet to /projects/aebc210b-e912-4df\n[nltk_data]     7-91ea-37e0f8451ece/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt to /projects/aebc210b-e912-4df7-\n[nltk_data]     91ea-37e0f8451ece/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to /project\n[nltk_data]     s/aebc210b-e912-4df7-91ea-37e0f8451ece/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n"},"1":{"name":"stdout","output_type":"stream","text":"[\" contemporary climate change includes both global warming and its impacts on earth's weather patterns.\", 'there have been previous periods of climate change, but the current changes are distinctly more rapid and not due to natural causes.', 'instead, they are caused by the emission of greenhouse gases, mostly carbon dioxide (co ) and methane.', 'burning fossil fuels for energy production creates most of these emissions.', 'certain agricultural practices, industrial processes, and forest loss are additional sources.', 'as their name suggests, these gases trap heat from sunlight near the earth’s surface, warming it over time.', 'due to climate change, deserts are expanding, while heat waves and wildfires are becoming more common.', 'increased warming in the arctic has contributed to melting permafrost, glacial retreat and sea ice loss.', 'higher temperatures are also causing more intense storms, droughts, and other weather extremes.', 'rapid environmental change in mountains, coral reefs, and the arctic is forcing many species to relocate or become extinct.']\n"}},"pos":9,"type":"cell"}
{"cell_type":"markdown","id":"0d127d","input":"# Natural Language Processing using NLTK","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"17cb49","input":"## Reflection\nHow important do you think proper preprocessing in NLP is?","pos":47,"type":"cell"}
{"cell_type":"markdown","id":"34387d","input":"## NLP Part 1 - Tokenization of paragraphs/sentences\n\nIn this section we are going to tokenize our sentences and words. If you aren't familiar with tokenization, we recommend looking up \"what is tokenization\". \n\nYou should also spend time on the [NLTK documentation](https://www.nltk.org/). If you're not sure how to do something, or get an error, it is best to google it first and ask questions as you go!\n\n","pos":7,"type":"cell"}
{"cell_type":"markdown","id":"43f883","input":"## NLP Part 0 - Get some Data!\n\nThis section's code is mostly given to you as a review for how you can scrape and manipulate data from the web. ","pos":3,"type":"cell"}
{"cell_type":"markdown","id":"4eaae7","input":"### Why did we do all this work?","pos":39,"type":"cell"}
{"cell_type":"markdown","id":"7aebaa","input":"","pos":1,"type":"cell"}
{"cell_type":"markdown","id":"88a08f","input":"## NLP Part 4 - POS Tagging\nParts of speech tagging is marking up a word in a text as corresponding to a particular part of speech, based on both its definition and its context.","pos":21,"type":"cell"}
{"cell_type":"markdown","id":"8a68ac","input":"## Testing our model","pos":33,"type":"cell"}
{"cell_type":"markdown","id":"b2b896","input":"## NLP Part 2 - Stopwords and Punctuation\n\nNow we are going to work to remove stopwords and punctuation from our data. Why do you think we are going to do this? Do some research if you don't know yet. \n\n","pos":10,"type":"cell"}
{"cell_type":"markdown","id":"b90bd6","input":"## NLP Part 3b - Lemmatization\nLemmatization considers the context and converts the word to its meaningful base form. There is a cool tutorial and definition of lemmatization in NLTK [here](https://www.geeksforgeeks.org/python-lemmatization-with-nltk/).","pos":18,"type":"cell"}
{"cell_type":"markdown","id":"c01498","input":"## NLP Part 3a - Stemming the words\nStemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form. There is an example below!","pos":14,"type":"cell"}
{"cell_type":"markdown","id":"cf8350","input":"","pos":48,"type":"cell"}
{"cell_type":"markdown","id":"ec6dc1","input":"## Training the Word2Vec model\n\nFor this part you may want to follow a guide [here](https://radimrehurek.com/gensim/models/word2vec.html). \n\n","pos":28,"type":"cell"}
{"cell_type":"markdown","id":"ed6212","input":"# Word2Vec Model Visualization","pos":24,"type":"cell"}
{"id":0,"time":1658859618068,"type":"user"}
{"last_load":1658859616399,"type":"file"}